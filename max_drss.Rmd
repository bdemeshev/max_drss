---
title: "Выбор сложности модели через максимизацию производной суммы квадратов остатков"
author: "Борис Демешев"
date: "14 мая 2015"
output:
  ioslides_presentation:
    keep_md: yes
  beamer_presentation:
    keep_tex: yes
  slidy_presentation: default
lang: russian
---

## Минимизация суммы квадратов остатков

Простая модель линейной регрессии:

\[
y_i = \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \varepsilon_i
\]


МНК: подбираем $\beta$ минимизируя $RSS$.


## Проблема с параметром сложности модели

\[
y_i = \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \varepsilon_i
\]

Однако нельзя подобрать $k$ минимизируя $RSS$. 

Чем больше сложность модели, $k$, тем меньше будет $RSS$.

## Дилемма сложность--сумма квадратов остатков универсальна

* LASSO

* Ridge-регрессия

* Классификационные деревья
 
* Ядерная оценка функции плотности

* Ядерная оценка в непараметрической регрессии

...

## Кратко о LASSO

Зафиксируем $k$.

Минимизируем по $\beta$ сумму квадратов остатков, оштрафованную на сложность модели:
\[
\min RSS + \lambda \cdot (|\beta_2| + |\beta_3| + \ldots + |\beta_k|)
\]

Чем больше штрафной параметр $\lambda$, тем (грубо говоря) ближе оптимальные $\beta^*$ будут к нулю.

Оптимизировать по $\lambda$ бессмысленно. 


## Известное решение --- кросс-валидация

1. Поделим имеющуюся выборку на 10 частей.

2. Зафиксируем некоторое значение штрафного параметра $\lambda$.

3. По подвыборке состоящей из всех частей кроме первой оценим $\beta$ с помощью LASSO.

4. Используя полученные $\hat{\beta}_{(1)}$ получим прогнозы для наблюдений из первой части.

5-6. Повторим шаги 3-4 для подвыборки, в который из всех наблюдений удалена вторая часть.

## Известное решение --- кросс-валидация

7-8. Повторим шаги 3-4 для подвыборки, в который из всех наблюдений удалена третья часть.

...

$n$. Получив прогнозы для каждого наблюдения посчитаем $RSS_{cv}(\lambda)$ для зафиксированного $\lambda$

$n+1$. Проделав шаги 2-$n$ для разных $\lambda$, выберем то, которое минимизирует $RSS_{cv}$.


## Новое решение --- максимизация производной $RSS$

1. Зафиксируем некоторое значение штрафного параметра $\lambda$.

2. По всей выборке оценим $\beta$ с помощью LASSO.

3. Проделав шаги 1-2 для разных $\lambda$, получим зависимость $RSS(\lambda)$, выберем то $\lambda$, которое максимизирует $dRSS(\lambda)/d\lambda$.


## Численный пример

```{r, include=FALSE}
library("dplyr")
library("glmnet")
library("ggplot2")
library("gridExtra")
library("pander")
```

Искусственные данные: $x_{ik} \sim N(0,1)$, $z_{ik} \sim N(0,1)$, $\varepsilon_i \sim N(0,1)$, 200 наблюдений:

\[
y_i = 2+ 3x_{i1} - 2x_{i2}+\varepsilon_i
\]

```{r, include=FALSE}
n <- 200
set.seed(42)
df <- data_frame(x1 = rnorm(n), x2=rnorm(n), z1=rnorm(n), z2=rnorm(n), eps=rnorm(n),
                 y=2+3*x1-2*x2+eps)
X <- model.matrix(~0+x1+x2+z1+z2,data=df)
y <- df$y
lambdas <- seq(50,0,by=-0.1)
model <- glmnet(X,y,alpha=0, lambda = lambdas)
```

С помощью LASSO оцениваем регрессию $y_i$ на $x_{i1}$, $x_{i2}$, $z_{i1}$, $z_{i2}$ при разных лямбда
```{r}
cv.fit <- cv.glmnet(X,y, lambda = lambdas)
```


## Результаты для разных штрафных коэффициентов

```{r, results="asis", echo=FALSE}
coefs <- data.frame(as.matrix(coef(cv.fit, s=c(0,0.5,1,2,10))))
coefs <- data.frame(names=rownames(coefs), coefs)
rownames(coefs) <- NULL
# coefs <- rbind(c(0,0.5,1,2,10),coefs)
# rownames(coefs)[1] <- "lambda"
colnames(coefs) <- c("Лямбда:","0 (МНК)","0.5","1","2","10")
pander(coefs, digits=3)
```

## Кросс-валидация выбирает классический МНК

```{r}
cv.fit$lambda.min
coef(cv.fit,s="lambda.min")
```

## Зависимость $RSS$ и $dRSS/d\lambda$ от $\lambda$ 

```{r, echo=FALSE}
#cv.fit$lambda.1se
#coef(cv.fit,s="lambda.1se")

preds <- predict(model,newx=X)
yy <- matrix(rep(y, times=length(lambdas)), nrow=length(y))
res_sq <- (yy-preds)^2
all_rss <- apply(res_sq,2,sum)



p1 <- qplot(lambdas, all_rss,xlab="Lambda",ylab="RSS")
drss <- diff(rev(all_rss))
lambdas_inc <- rev(lambdas)[-1]
p2 <- qplot(lambdas_inc,drss,xlab="Lambda",ylab="delta RSS")

grid.arrange(p1, p2, ncol=2)
```


## Оценка лямбда максимизацией $dRSS/d\lambda$

```{r}
lambda.mdr <- lambdas_inc[drss==max(drss)]
lambda.mdr
coef(cv.fit, s=lambda.mdr)
```

## Потери по сравнению с "идеальным" решением


```{r, results="asis", echo=FALSE}
preds2 <- cbind(2+3*df$x1-2*df$x2, predict(model,newx=X,s=c(cv.fit$lambda.min, lambda.mdr)),df$y)

pdf <- data.frame(preds2)
colnames(pdf) <- c("y_b","y_cv","y_mdr","y")

tss <- var(pdf$y)*(length(pdf$y) - 1)
rss_b <- sum((pdf$y_b-pdf$y)^2)
rss_cv <- sum((pdf$y_cv-pdf$y)^2)
rss_mdr <- sum((pdf$y_mdr-pdf$y)^2)


losses <- data_frame(model=c("Идеальная","Кросс-валидация", "Максимум производной"), 
                     lambda=c(NA,cv.fit$lambda.min, lambda.mdr),
                     RSS_TSS=c(rss_b/tss,rss_cv/tss,rss_mdr/tss), 
                     scorr2=c(cor(pdf$y_b, pdf$y)^2, cor(pdf$y_cv, pdf$y)^2, cor(pdf$y_mdr, pdf$y)^2 ),
                     n_coefs=c(2,4,2))
colnames(losses) <- c("Модель","Лямбда","RSS/TSS", "sCorr^2","Параметры")
pander(losses)
```



## Свойства метода

* Метод зависит от выбора целевого показателя. 

Например, вместо суммы квадратов остатков можно взять сумму квадратов модулей.

* У функции $dRSS/d\lambda$ может быть несколько локальных максимумов 


Эти слады доступны по ссылке [goo.gl/GFMeG3](https://github.com/bdemeshev/max_drss)




