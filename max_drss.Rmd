---
title: "Выбор сложности модели через максимизацию производной суммы квадратов остатков"
author: "Борис Демешев"
date: "14 мая 2015"
output:
  slidy_presentation: default
  ioslides_presentation:
    keep_md: yes
lang: russian
---

## Минимизация суммы квадратов остатков

Простая модель линейной регрессии:

\[
y_i = \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \varepsilon_i
\]


МНК: подбираем $\beta$ минимизируя $RSS$.


## Проблема с параметром сложности модели

\[
y_i = \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \varepsilon_i
\]

Однако нельзя подобрать $k$ минимизируя $RSS$. 

Чем больше сложность модели, $k$, тем меньше будет $RSS$.

## Дилемма сложность--сумма квадратов остатков универсальна

* LASSO

* Ridge-регрессия

* Классификационные деревья
 
* Ядерная оценка функции плотности

* Ядерная оценка в непараметрической регрессии

...

## Кратко о LASSO

Зафиксируем $k$.

Минимизируем по $\beta$ сумму квадратов остатков, оштрафованную на сложность модели:
\[
\min RSS + \lambda \cdot (|\beta_2| + |\beta_3| + \ldots + |\beta_k|)
\]

Чем больше штрафной параметр $\lambda$, тем (грубо говоря) ближе оптимальные $\beta^*$ будут к нулю.

Оптимизировать по $\lambda$ бессмысленно. 


## Известное решение --- кросс-валидация

1. Поделим имеющуюся выборку на 10 частей.

2. Зафиксируем некоторое значение штрафного параметра $\lambda$.

3. По подвыборке состоящей из всех частей кроме первой оценим $\beta$ с помощью LASSO.

4. Используя полученные $\hat{\beta}_{(1)}$ получим прогнозы для наблюдений из первой части.

5-6. Повторим шаги 3-4 для подвыборки, в который из всех наблюдений удалена вторая часть.

## Известное решение --- кросс-валидация

7-8. Повторим шаги 3-4 для подвыборки, в который из всех наблюдений удалена третья часть.

...

$n$. Получив прогнозы для каждого наблюдения посчитаем $RSS_{cv}(\lambda)$ для зафиксированного $\lambda$

$n+1$. Проделав шаги 2-$n$ для разных $\lambda$, выберем то, которое минимизирует $RSS_{cv}$.


## Новое решение --- максимизация производной $RSS$

1. Зафиксируем некоторое значение штрафного параметра $\lambda$.

2. По всей выборке оценим $\beta$ с помощью LASSO.

3. Проделав шаги 1-2 для разных $\lambda$, получим зависимость $RSS(\lambda)$, выберем то $\lambda$, которое максимизирует $dRSS(\lambda)/d\lambda$.


## Численный пример

```{r, include=FALSE}
library("dplyr")
library("glmnet")
library("ggplot2")
library("gridExtra")
```

Искусственные данные: $x_{ik} \sim N(0,1)$, $z_{ik} \sim N(0,1)$, $\varepsilon_i \sim N(0,1)$, 200 наблюдений:

\[
y_i = 2+ 3x_{i1} - 2x_{i2}+\varepsilon_i
\]

```{r, include=FALSE}
n <- 200
set.seed(42)
df <- data_frame(x1 = rnorm(n), x2=rnorm(n), z1=rnorm(n), z2=rnorm(n), eps=rnorm(n),
                 y=2+3*x1-2*x2+eps)
X <- model.matrix(~0+x1+x2+z1+z2,data=df)
y <- df$y
lambdas <- seq(50,0,by=-0.1)
model <- glmnet(X,y,alpha=0, lambda = lambdas)
```

С помощью LASSO оцениваем регрессию $y_i$ на $x_{i1}$, $x_{i2}$, $z_{i1}$, $z_{i2}$ при разных лямбда
```{r}
cv.fit <- cv.glmnet(X,y, lambda = lambdas)
```



## Результаты для разных штрафных коэффициентов

тут табличка

## Кросс-валидация выбирает классический МНК

```{r}
cv.fit$lambda.min
coef(cv.fit,s="lambda.min")
```

## Зависимость $RSS$ и $dRSS/d\lambda$ от $\lambda$ 

```{r, echo=FALSE}
#cv.fit$lambda.1se
#coef(cv.fit,s="lambda.1se")

preds <- predict(model,newx=X)
yy <- matrix(rep(y, times=length(lambdas)), nrow=length(y))
res_sq <- (yy-preds)^2
all_rss <- apply(res_sq,2,sum)



p1 <- qplot(lambdas, all_rss,xlab="Lambda",ylab="RSS")
drss <- diff(rev(all_rss))
lambdas_inc <- rev(lambdas)[-1]
p2 <- qplot(lambdas_inc,drss,xlab="Lambda",ylab="delta RSS")

grid.arrange(p1, p2, ncol=2)
```


## Оценка лямбда максимизацией $dRSS/d\lambda$

```{r}
lambda.mdr <- lambdas_inc[drss==max(drss)]
lambda.mdr
coef(cv.fit, s=lambda.mdr)
```

* Потери по сравнению с "идеальным" решением

тут привести потери по ESS или R2



## Свойства метода

* Метод зависит от выбора целевого показателя. 

Например, вместо суммы квадратов остатков можно взять сумму квадратов модулей.

* У функции $dRSS/d\lambda$ может быть несколько локальных максимумов 


Эти слады доступны по ссылке [goo.gl/GFMeG3](https://github.com/bdemeshev/max_drss)




