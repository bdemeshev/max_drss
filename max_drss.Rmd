---
title: "Выбор сложности модели через максимизацию производной суммы квадратов остатков"
author: "Борис Демешев"
date: "14 мая 2015"
output:
  ioslides_presentation:
    keep_md: yes
lang: russian
---

## Минимизация суммы квадратов остатков

Простая модель линейной регрессии:

\[
y_i = \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \varepsilon_i
\]


МНК: подбираем $\beta$ минимизируя $RSS$.


## Проблема с параметром сложности модели

\[
y_i = \beta_1 + \beta_2 x_{i2}+ \ldots + \beta_k x_{ik} + \varepsilon_i
\]

Однако нельзя подобрать $k$ минимизируя $RSS$. 

Чем больше сложность модели, $k$, тем меньше будет $RSS$.

## Дилемма сложность--сумма квадратов остатков универсальна

* LASSO

* Ridge-регрессия

* Классификационные деревья
 
* Ядерная оценка функции плотности

* Ядерная оценка в непараметрической регрессии

...

## Кратко о LASSO

Зафиксируем $k$.

Минимизируем по $\beta$ сумму квадратов остатков, оштрафованную на сложность модели:
\[
\min RSS + \lambda \cdot (|\beta_2| + |\beta_3| + \ldots + |\beta_k|)
\]

Чем больше штрафной параметр $\lambda$, тем (грубо говоря) ближе оптимальные $\beta^*$ будут к нулю.

Оптимизировать по $\lambda$ бессмысленно. 


## Известное решение --- кросс-валидация

1. Поделим имеющуюся выборку на 10 частей.

2. Зафиксируем некоторое значение штрафного параметра $\lambda$.

3. По подвыборке состоящей из всех частей кроме первой оценим $\beta$ с помощью LASSO.

4. Используя полученные $\hat{\beta}_{(1)}$ получим прогнозы для наблюдений из первой части.

5-6. Повторим шаги 3-4 для подвыборки, в который из всех наблюдений удалена вторая часть.

## Известное решение --- кросс-валидация

7-8. Повторим шаги 3-4 для подвыборки, в который из всех наблюдений удалена третья часть.

...

$n$. Получив прогнозы для каждого наблюдения посчитаем $RSS_{cv}(\lambda)$ для зафиксированного $\lambda$

$n+1$. Проделав шаги 2-$n$ для разных $\lambda$, выберем то, которое минимизирует $RSS_{cv}$.


## Новое решение --- максимизация производной $RSS$

1. Зафиксируем некоторое значение штрафного параметра $\lambda$.

2. По всей выборке оценим $\beta$ с помощью LASSO.

3. Проделав шаги 1-2 для разных $\lambda$, получим зависимость $RSS(\lambda)$, выберем то $\lambda$, которое максимизирует $dRSS(\lambda)/d\lambda$.


## Численный пример



## Если понравилось

Эти слады доступны по ссылке []()




